{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../../SenWave-BERT/SenWave-BERT\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"thank you\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits)\n",
    "\n",
    "predicted_labels = (probs > 0.5).int()\n",
    "\n",
    "for i, val in enumerate(predicted_labels[0]):\n",
    "    if val == 1:\n",
    "        label = model.config.id2label[i]\n",
    "        print(f\"Predicted label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits)\n",
    "            \n",
    "        predicted_labels = (probs > 0.5).int()  # multilabel predictions\n",
    "        \n",
    "        predicted_labels_list = []\n",
    "        for i, val in enumerate(predicted_labels[0]):\n",
    "            if val == 1:\n",
    "                label = model.config.id2label[i]\n",
    "                predicted_labels_list.append(label)\n",
    "        \n",
    "        # if no labels are predicted (<.5) then just take argmax\n",
    "        if not predicted_labels_list:\n",
    "            predicted_label_idx = torch.argmax(probs, dim=-1).item()  \n",
    "            label = model.config.id2label[predicted_label_idx]\n",
    "            return [label]  \n",
    "        \n",
    "        return predicted_labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Dataset/tweets/transformed_final.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleaned_transform</th>\n",
       "      <th>labels</th>\n",
       "      <th>batch</th>\n",
       "      <th>transformed_tweets_groq</th>\n",
       "      <th>groq_transformed</th>\n",
       "      <th>transformed_tweets_gemini</th>\n",
       "      <th>gemini_transformed</th>\n",
       "      <th>transformed_tweets_gpt</th>\n",
       "      <th>...</th>\n",
       "      <th>before_hatebert_pred</th>\n",
       "      <th>after_hatebert_pred_groq</th>\n",
       "      <th>after_hatebert_pred_gemini</th>\n",
       "      <th>after_hatebert_pred_gpt4o</th>\n",
       "      <th>after_hatebert_pred_deepseek</th>\n",
       "      <th>hate_keyword_before</th>\n",
       "      <th>hate_keyword_groq</th>\n",
       "      <th>hate_keyword_gemini</th>\n",
       "      <th>hate_keyword_gpt</th>\n",
       "      <th>hate_keyword_deepseek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'If you bother to do the maths then in actual...</td>\n",
       "      <td>bother math actual fact socalled reformer isla...</td>\n",
       "      <td>if you bother to do the maths then in actual f...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1</td>\n",
       "      <td>\"If you take the time to do the math, you'll f...</td>\n",
       "      <td>1</td>\n",
       "      <td>A mathematical analysis reveals that those cla...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"If you take the time to do the calculations, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Muslims be like: The Quran is true, it says ...</td>\n",
       "      <td>muslim like quran true say quran</td>\n",
       "      <td>muslims be like the quran is true it says so i...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I'm fascinated by the Quran's teachings and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Some Muslims express their faith by citing the...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Some Muslims believe that the Quran is true b...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That painting by Jim Carrey of Sharia Huckabal...</td>\n",
       "      <td>painting jim carrey sharia huckabaloney spitti...</td>\n",
       "      <td>that painting by jim carrey of sharia huckabal...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I'm struck by Jim Carrey's depiction of Shari...</td>\n",
       "      <td>1</td>\n",
       "      <td>I find Jim Carrey's painting to be a striking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"That painting by Jim Carrey captures a striki...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b\"So I say.. fck Islam it should be banned in ...</td>\n",
       "      <td>say fck islam banned uk go belief womens right</td>\n",
       "      <td>so i say fck islam it should be banned in the ...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I believe Islamic laws and beliefs often clas...</td>\n",
       "      <td>1</td>\n",
       "      <td>I believe that certain Islamic practices confl...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I believe there should be a discussion about ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"Theresa May has her head so far up the Musla...</td>\n",
       "      <td>theresa may head far muslamic as doesnt need s...</td>\n",
       "      <td>theresa may has her head so far up the muslami...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Concerned about PM Theresa May's response to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I strongly disagree with Theresa May's policies.</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Theresa May seems to have strong alignment wi...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>b'Why does this Globalist Pope meddle in suppo...</td>\n",
       "      <td>globalist pope meddle supporting muslim invasi...</td>\n",
       "      <td>why does this globalist pope meddle in support...</td>\n",
       "      <td>Disciminatory</td>\n",
       "      <td>20</td>\n",
       "      <td>\"I'm concerned about the Pope's influence on g...</td>\n",
       "      <td>1</td>\n",
       "      <td>I question the Pope's focus on supporting Musl...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Why does this globalist Pope focus on support...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>b'RT @BeckysTidePods: FLASHBACK: for the selec...</td>\n",
       "      <td>flashback selective memory whining hypocrite b...</td>\n",
       "      <td>flashback for the selective memory whining hyp...</td>\n",
       "      <td>Disciminatory</td>\n",
       "      <td>20</td>\n",
       "      <td>\"Remembering the contrast between past actions...</td>\n",
       "      <td>1</td>\n",
       "      <td>Reflecting on past events, it's worth remember...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Flashback for those with selective memory: Th...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>b'RT @chriscol22nd: Illinois: Two Muslims indi...</td>\n",
       "      <td>illinois two muslim indicted spying behalf ira...</td>\n",
       "      <td>illinois two muslims indicted for spying on be...</td>\n",
       "      <td>Disciminatory</td>\n",
       "      <td>20</td>\n",
       "      <td>\"Breaking: Two individuals in Illinois have be...</td>\n",
       "      <td>1</td>\n",
       "      <td>Two individuals in Illinois have been indicted...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Two individuals in Illinois have been indicte...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>b'@najwazebian Wishing all young Lebanese fema...</td>\n",
       "      <td>wishing young lebanese female teacher living l...</td>\n",
       "      <td>wishing all young lebanese female teachers liv...</td>\n",
       "      <td>Disciminatory</td>\n",
       "      <td>20</td>\n",
       "      <td>\"Wishing all young Lebanese female teachers li...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wishing all young Lebanese female teachers in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Wishing all young Lebanese female teachers li...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>b\"They used to put Traitors' heads on spikes o...</td>\n",
       "      <td>used put traitor head spike old london bridge ...</td>\n",
       "      <td>they used to put traitors heads on spikes on o...</td>\n",
       "      <td>Disciminatory</td>\n",
       "      <td>20</td>\n",
       "      <td>\"A historical reminder to appreciate the impor...</td>\n",
       "      <td>1</td>\n",
       "      <td>Historically, traitors were executed near Lond...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"In the past, those considered traitors were d...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets  \\\n",
       "0    b'If you bother to do the maths then in actual...   \n",
       "1    b'Muslims be like: The Quran is true, it says ...   \n",
       "2    That painting by Jim Carrey of Sharia Huckabal...   \n",
       "3    b\"So I say.. fck Islam it should be banned in ...   \n",
       "4    b\"Theresa May has her head so far up the Musla...   \n",
       "..                                                 ...   \n",
       "395  b'Why does this Globalist Pope meddle in suppo...   \n",
       "396  b'RT @BeckysTidePods: FLASHBACK: for the selec...   \n",
       "397  b'RT @chriscol22nd: Illinois: Two Muslims indi...   \n",
       "398  b'@najwazebian Wishing all young Lebanese fema...   \n",
       "399  b\"They used to put Traitors' heads on spikes o...   \n",
       "\n",
       "                                               cleaned  \\\n",
       "0    bother math actual fact socalled reformer isla...   \n",
       "1                     muslim like quran true say quran   \n",
       "2    painting jim carrey sharia huckabaloney spitti...   \n",
       "3       say fck islam banned uk go belief womens right   \n",
       "4    theresa may head far muslamic as doesnt need s...   \n",
       "..                                                 ...   \n",
       "395  globalist pope meddle supporting muslim invasi...   \n",
       "396  flashback selective memory whining hypocrite b...   \n",
       "397  illinois two muslim indicted spying behalf ira...   \n",
       "398  wishing young lebanese female teacher living l...   \n",
       "399  used put traitor head spike old london bridge ...   \n",
       "\n",
       "                                     cleaned_transform         labels  batch  \\\n",
       "0    if you bother to do the maths then in actual f...       Religion      1   \n",
       "1    muslims be like the quran is true it says so i...       Religion      1   \n",
       "2    that painting by jim carrey of sharia huckabal...       Religion      1   \n",
       "3    so i say fck islam it should be banned in the ...       Religion      1   \n",
       "4    theresa may has her head so far up the muslami...       Religion      1   \n",
       "..                                                 ...            ...    ...   \n",
       "395  why does this globalist pope meddle in support...  Disciminatory     20   \n",
       "396  flashback for the selective memory whining hyp...  Disciminatory     20   \n",
       "397  illinois two muslims indicted for spying on be...  Disciminatory     20   \n",
       "398  wishing all young lebanese female teachers liv...  Disciminatory     20   \n",
       "399  they used to put traitors heads on spikes on o...  Disciminatory     20   \n",
       "\n",
       "                               transformed_tweets_groq  groq_transformed  \\\n",
       "0    \"If you take the time to do the math, you'll f...                 1   \n",
       "1    \"I'm fascinated by the Quran's teachings and t...                 1   \n",
       "2    \"I'm struck by Jim Carrey's depiction of Shari...                 1   \n",
       "3    \"I believe Islamic laws and beliefs often clas...                 1   \n",
       "4    \"Concerned about PM Theresa May's response to ...                 1   \n",
       "..                                                 ...               ...   \n",
       "395  \"I'm concerned about the Pope's influence on g...                 1   \n",
       "396  \"Remembering the contrast between past actions...                 1   \n",
       "397  \"Breaking: Two individuals in Illinois have be...                 1   \n",
       "398  \"Wishing all young Lebanese female teachers li...                 1   \n",
       "399  \"A historical reminder to appreciate the impor...                 1   \n",
       "\n",
       "                             transformed_tweets_gemini  gemini_transformed  \\\n",
       "0    A mathematical analysis reveals that those cla...                   1   \n",
       "1    Some Muslims express their faith by citing the...                   1   \n",
       "2    I find Jim Carrey's painting to be a striking ...                   1   \n",
       "3    I believe that certain Islamic practices confl...                   1   \n",
       "4     I strongly disagree with Theresa May's policies.                   1   \n",
       "..                                                 ...                 ...   \n",
       "395  I question the Pope's focus on supporting Musl...                   1   \n",
       "396  Reflecting on past events, it's worth remember...                   1   \n",
       "397  Two individuals in Illinois have been indicted...                   1   \n",
       "398  Wishing all young Lebanese female teachers in ...                   1   \n",
       "399  Historically, traitors were executed near Lond...                   1   \n",
       "\n",
       "                                transformed_tweets_gpt  ...  \\\n",
       "0    \"If you take the time to do the calculations, ...  ...   \n",
       "1    \"Some Muslims believe that the Quran is true b...  ...   \n",
       "2    \"That painting by Jim Carrey captures a striki...  ...   \n",
       "3    \"I believe there should be a discussion about ...  ...   \n",
       "4    \"Theresa May seems to have strong alignment wi...  ...   \n",
       "..                                                 ...  ...   \n",
       "395  \"Why does this globalist Pope focus on support...  ...   \n",
       "396  \"Flashback for those with selective memory: Th...  ...   \n",
       "397  \"Two individuals in Illinois have been indicte...  ...   \n",
       "398  \"Wishing all young Lebanese female teachers li...  ...   \n",
       "399  \"In the past, those considered traitors were d...  ...   \n",
       "\n",
       "     before_hatebert_pred after_hatebert_pred_groq  \\\n",
       "0                       1                        1   \n",
       "1                       1                        1   \n",
       "2                       1                        1   \n",
       "3                       1                        1   \n",
       "4                       1                        1   \n",
       "..                    ...                      ...   \n",
       "395                     1                        1   \n",
       "396                     1                        1   \n",
       "397                     1                        1   \n",
       "398                     1                        1   \n",
       "399                     1                        1   \n",
       "\n",
       "     after_hatebert_pred_gemini  after_hatebert_pred_gpt4o  \\\n",
       "0                             1                          1   \n",
       "1                             1                          1   \n",
       "2                             1                          1   \n",
       "3                             1                          1   \n",
       "4                             1                          1   \n",
       "..                          ...                        ...   \n",
       "395                           1                          1   \n",
       "396                           1                          1   \n",
       "397                           1                          1   \n",
       "398                           1                          1   \n",
       "399                           1                          1   \n",
       "\n",
       "     after_hatebert_pred_deepseek  hate_keyword_before  hate_keyword_groq  \\\n",
       "0                               1                    0                  0   \n",
       "1                               1                    0                  0   \n",
       "2                               1                    0                  0   \n",
       "3                               1                    0                  0   \n",
       "4                               1                    0                  0   \n",
       "..                            ...                  ...                ...   \n",
       "395                             1                    0                  0   \n",
       "396                             1                    0                  0   \n",
       "397                             1                    0                  0   \n",
       "398                             1                    0                  0   \n",
       "399                             1                    0                  0   \n",
       "\n",
       "     hate_keyword_gemini  hate_keyword_gpt  hate_keyword_deepseek  \n",
       "0                      0                 0                      0  \n",
       "1                      0                 0                      0  \n",
       "2                      0                 0                      0  \n",
       "3                      0                 0                      0  \n",
       "4                      0                 0                      0  \n",
       "..                   ...               ...                    ...  \n",
       "395                    0                 0                      0  \n",
       "396                    0                 0                      0  \n",
       "397                    0                 0                      0  \n",
       "398                    0                 0                      0  \n",
       "399                    0                 0                      0  \n",
       "\n",
       "[400 rows x 23 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweets', 'cleaned', 'cleaned_transform', 'labels', 'batch',\n",
       "       'transformed_tweets_groq', 'groq_transformed',\n",
       "       'transformed_tweets_gemini', 'gemini_transformed',\n",
       "       'transformed_tweets_gpt', 'gpt_transformed',\n",
       "       'transformed_tweets_deepseek', 'deepseek_transformed',\n",
       "       'before_hatebert_pred', 'after_hatebert_pred_groq',\n",
       "       'after_hatebert_pred_gemini', 'after_hatebert_pred_gpt4o',\n",
       "       'after_hatebert_pred_deepseek', 'hate_keyword_before',\n",
       "       'hate_keyword_groq', 'hate_keyword_gemini', 'hate_keyword_gpt',\n",
       "       'hate_keyword_deepseek'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cleaned_transform', 'transformed_tweets_groq', 'transformed_tweets_gemini', 'transformed_tweets_gpt', 'transformed_tweets_deepseek']\n",
    "summary = {}\n",
    "for col in columns:\n",
    "    all_labels = df[col].apply(lambda x: predict_sentiment(x))\n",
    "    flat_labels = [label for labels in all_labels for label in labels]\n",
    "    label_counts = Counter(flat_labels)\n",
    "    summary[col] = label_counts\n",
    "\n",
    "summary_df = pd.DataFrame(summary).fillna(0).astype(int).T \n",
    "summary_df.index.name = \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Joking</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Thankful</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cleaned_transform</th>\n",
       "      <td>259</td>\n",
       "      <td>165</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_tweets_groq</th>\n",
       "      <td>145</td>\n",
       "      <td>55</td>\n",
       "      <td>190</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_tweets_gemini</th>\n",
       "      <td>163</td>\n",
       "      <td>59</td>\n",
       "      <td>118</td>\n",
       "      <td>73</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_tweets_gpt</th>\n",
       "      <td>163</td>\n",
       "      <td>72</td>\n",
       "      <td>111</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_tweets_deepseek</th>\n",
       "      <td>164</td>\n",
       "      <td>58</td>\n",
       "      <td>133</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Annoyed  Joking  Optimistic  Official report  \\\n",
       "Model                                                                       \n",
       "cleaned_transform                259     165          33               30   \n",
       "transformed_tweets_groq          145      55         190               36   \n",
       "transformed_tweets_gemini        163      59         118               73   \n",
       "transformed_tweets_gpt           163      72         111               65   \n",
       "transformed_tweets_deepseek      164      58         133               54   \n",
       "\n",
       "                             Anxious  Sad  Denial  Thankful  \n",
       "Model                                                        \n",
       "cleaned_transform                  9   10       6         0  \n",
       "transformed_tweets_groq            5    8       5         1  \n",
       "transformed_tweets_gemini         15   18       6         1  \n",
       "transformed_tweets_gpt             5   15      10         1  \n",
       "transformed_tweets_deepseek        6   13      15         0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(\"../../Dataset/tweets/sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv(\"../../Dataset/tweets/sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Joking</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Thankful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>165</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>55</td>\n",
       "      <td>190</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>59</td>\n",
       "      <td>118</td>\n",
       "      <td>73</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163</td>\n",
       "      <td>72</td>\n",
       "      <td>111</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>58</td>\n",
       "      <td>133</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Annoyed  Joking  Optimistic  Official report  Anxious  Sad  Denial  \\\n",
       "0      259     165          33               30        9   10       6   \n",
       "1      145      55         190               36        5    8       5   \n",
       "2      163      59         118               73       15   18       6   \n",
       "3      163      72         111               65        5   15      10   \n",
       "4      164      58         133               54        6   13      15   \n",
       "\n",
       "   Thankful  \n",
       "0         0  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
